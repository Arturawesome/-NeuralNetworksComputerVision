{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNV+GANvJTQmo6o87Ax+vC3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3IJWAj7Psxlh"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n"]},{"cell_type":"markdown","source":["Сгенерируем датасет Игрушка Дьявола"],"metadata":{"id":"e4aHn7v7tgS_"}},{"cell_type":"code","source":["# код для генерации взят из Стэнфордсокго курса:\n","# http://cs231n.github.io/neural-networks-case-study/#linear\n","\n","N = 100\n","D = 2\n","K = 3\n","X = np.zeros((N * K, D))\n","y = np.zeros(N * K, dtype='uint8')\n","\n","for j in range(K):\n","    ix = range(N * j,N * (j + 1))\n","    r = np.linspace(0.0, 1, N)\n","    t = np.linspace(j * 4, (j + 1) * 4,N) + np.random.randn(N) * 0.2 # theta\n","    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n","    y[ix] = j\n","\n","# Отрисовочная магия, снова взято из:\n","# http://cs231n.github.io/neural-networks-case-study/#linear\n","\n","plt.figure(figsize=(10, 8))\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.rainbow)\n","\n","plt.title('Игрушка дьявола', fontsize=15)\n","plt.xlabel('$x$', fontsize=14)\n","plt.ylabel('$y$', fontsize=14)\n","plt.show();"],"metadata":{"id":"3MpbCPiPtcWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in newral nets used Tensor\n","X = torch.FloatTensor(X)  #points coordinates for X and Y axes -> float \n","y = torch.LongTensor(y) # index of classes\n","y\n"],"metadata":{"id":"G0oH7Rzxt9qS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Сейчас мы хотим научиться самостоятельно создавать наследников nn.Module. До этого мы делали нейросети с помощью класса nn.Sequential, попробуем построить такую же сеть, как на прошлом семинаре, но самостоятельно."],"metadata":{"id":"74sG1Xbqumv_"}},{"cell_type":"code","source":["# СЕТЬ С ПРОШЛОГО СЕМИНАРА\n","\n","# D_in - размерность входа (количество признаков у объекта);\n","# H - размерность скрытых слоёв;\n","# D_out - размерность выходного слоя (суть - количество классов)\n","D_in, H, D_out = 2, 100, 3\n","\n","two_layer_net = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H), # set layers\n","    torch.nn.ReLU(),  # set activate fucntion\n","    torch.nn.Linear(H, D_out), # set the last finul layer\n","    torch.nn.Softmax()   # set softmax for classify task. This fucntion using only in the last layer\n","\n",")\n"],"metadata":{"id":"_yfk_HUnubra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Что такое модуль и как он устроен? Во-первых, модуль это такой строительный блок для нейронок, с помощью модуля можно задать любую дифференциируемую по своему параметру функцию. Применяются модули так же, как и обычные функции с синтаксисом\n","> module_instance(var1, var2)\n","\n","При этом внутри вызывается функция forward с теми же аргументами, а ее выход возвращается как результат вызова модуля. Зачем же нужно так странно оборачивать обычные функции в модули? \n","\n","* Это позволяет очень удобно следить за параметрами, которые надо изменять. Когда мы хоти получить все параметры можно просто рекурсивно пройтись по всем полям модели, посмотреть, какие из ни параметры сами по себе, а какие являются модулями и содрежат параметры внутри, а потом все это собрать. \n","\n","_По этой причине если вы используете внутри своего модуля какие-то еще модули их надо класть просто в поле класса, если это единичный модуль, и в класс **nn.ModuleList** или **nn.ModuleDict**, если у вас список или словарь используемых модулей. Если же в модели у вас есть како-то собственный вес, то недостаточно положить тензор в поле класса, его надо обернуть в **nn.Parameter, nn.ParameterList** или **nn.ParameterDict** в зависимотси от того, что именно у вас._\n","\n","* Такая организация позволяет достаточно безболезненно расширять PyTorch и писать для него свои функции, которые нельзя выразить композицией уже существующих. Пригождается это редко, поэтому сегодня мы не будем писать свое расширение.\n","\n","* Код, разделенный на модули, это просто красиво."],"metadata":{"id":"uQ5V7481xrwx"}},{"cell_type":"code","source":["class MyModule(nn.Module):\n","  def __init__(self):\n","    super().__init__()  # constructir for nn.Module\n","    self.f_lin = nn.Linear(D_in, H) # layer for using\n","    self.s_lin = nn.Linear(H, D_out)  # layer for using\n","  \n","  # in order to set the training we need:\n","  # 1 forwerdpas прямое распространиен сигнала\n","  # 2 backwardpas обратное распространение сигнала\n","  def forward(self, X):\n","    # induce the first layer and cover it with the non-linear function \"Relu\"\n","    X = F.relu(self.f_lin(X))\n","\n","    # induced the second layer and cover it with softmax fucntion for getting probabilty\n","    return F.softmax(self.s_lin(X))\n","\n","# set our newrel net\n","model  = MyModule()\n","loss_fn = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(model.parameters(), 1e-1)\n","list(model.parameters())"],"metadata":{"id":"RgQCrp_yxm1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Поговорим немного подробнее о `softmax` и `CrossEntropyLoss`. \n","\n","Напоминание: softmax-функция выглядит следующим образом\n","$$Softmax(x) = \\begin{pmatrix} \n","\\dfrac{e^{x_1}}{\\sum\\limits_{i=0}^{m} e^{x_i}},\\; \\dots,\\; \\dfrac{e^{x_m}}{\\sum\\limits_{i=0}^{m} e^{x_i}}\n","\\end{pmatrix} $$\n","\n","Таким образом, после применения softmax-функции мы получили вектор чисел из интервала $(0, 1)$, которые мы будем интерпретировать как вероятности классов.\n","\n","Аналогично тому, как мы обощили сигмоиду на многоклассовый случай и получили softmax, можно обобщить и функцию потерь, получив _кросс-энтропию_:\n","\n","$$CrossEntropy(x) = -\\sum\\limits_{i=0}^m y_i \\cdot log(Softmax(x)_i), $$\n","\n","где $y_i$ – истинная метка класса ($1$ если $x$ принадлежит $i$-ому классу и $0$ иначе)"],"metadata":{"id":"D-1oUa9z0yPC"}},{"cell_type":"code","source":["losses = []\n","for t in range(100):\n","  # prediction, the model automatic induced the forward function\n","  y_pred = model(X)\n","\n","  # calc loss\n","  loss = loss_fn(y_pred, y)\n","  losses.append(loss.item())\n","\n","  optim.zero_grad()\n","  \n","  loss.backward()\n","  # step of optimizer\n","  optim.step()\n"],"metadata":{"id":"uwZVncOK1O3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(losses)"],"metadata":{"id":"7XklyJOI3uAi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Теперь посмотрим, что будет, если не положить используемые внутри слои в self"],"metadata":{"id":"NENNylVT438r"}},{"cell_type":"code","source":["# Сеть без параметров\n","\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear_layers = [nn.Linear(D_in, H), nn.Linear(H, D_out)]\n","        self.my_useless_bias = torch.ones(1, H, requires_grad=True)\n","        self.more_of_my_useless_biases = [\n","            torch.ones(1, H, requires_grad=True),\n","            torch.ones(1, H, requires_grad=True),\n","            torch.ones(1, H, requires_grad=True)\n","        ]\n","        \n","    def forward(self, X):\n","        X = F.relu(self.linear_layers[0](X))\n","        X += self.my_useless_bias\n","        return F.softmax(self.linear_layers[1](X))\n","    \n","model = MyModule()\n","list(model.parameters())"],"metadata":{"id":"DlhTh8CA4ghP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Какие параметры есть у модели?\n","### Поля класса должны быть относледованы от nn.Module\n","Как мы и ожидали параметров нет. Исправим это."],"metadata":{"id":"9tB3hwke5cY0"}},{"cell_type":"code","source":["# Исправленная сеть с параметрами\n","# для использования своих моделей (их улучшения итд) дополнения стоит оборачивать в функции nn.Something\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear_layers = nn.ModuleList([nn.Linear(D_in, H), nn.Linear(H, D_out)])\n","        self.my_useless_bias = nn.Parameter(torch.ones(1, H, requires_grad=True))\n","        self.more_of_my_useless_biases = nn.ParameterList([\n","            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n","            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n","            nn.Parameter(torch.ones(1, H, requires_grad=True))\n","        ])\n","        \n","    def forward(self, X):\n","        X = F.relu(self.linear_layers[0](X))\n","        X += self.my_useless_bias\n","        for b in self.more_of_my_useless_biases:\n","            X += b\n","        return F.softmax(self.linear_layers[1](X))\n","    \n","model = MyModule()\n","list(model., requires_grad=True())"],"metadata":{"id":"-JIsYVYP5UMU"},"execution_count":null,"outputs":[]}]}